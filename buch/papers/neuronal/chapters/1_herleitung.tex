%
% 1_herleitung.tex -- Herleitung der Methode
%
% (c) 2025 Roman Cvijanovic & Nicola Dall'Acqua, Hochschule Rapperswil
%
% !TEX root = ../../buch.tex
% !TEX encoding = UTF-8
%

\section{Herleitung der Methode\label{neuronal:section:herleitung}}
\kopfrechts{Herleitung der Methode}

Im Folgenden wird die Methode zum Lösen von Feldgleichungen mittels eines neuronalen Netzes theoretisch hergeleitet.
Dies wird anhand des Beispiels der Wellengleichung in zwei räumlichen Dimensionen gemacht
\begin{equation}
    \frac{\partial^2 u}{\partial t^2} = c^2 \left( \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} \right).
    \label{neuronal:wellengleichung}
\end{equation}

Wobei \( u(x, y, t) \) die z-Koordinate am Punkt \( (x, y) \) zum Zeitpunkt \( t \) darstellt.
Anders ausgedrückt ist \( u \) eine Oberfläche, welche sich im Laufe der Zeit ändert und somit eine Welle modelliert. 
Zudem ist \( c \in \mathbb{R} \) eine Konstante und stellt die Verbreitungsgeschwindigkeit der Welle dar.

Weiter sei das neuronale Netzwerk gegeben als
\begin{equation}
    \hat{u}(x, y, t; \vartheta).
    \label{neuronal:nn}
\end{equation}
Das Netz hängt von den gleichen Variablen ab wie \( u \).
Zusätzlich besitzt es einen Vektor \( \vartheta \in \mathbb{R}^n \) der n \emph{trainierbaren Parameter}.
Das Ziel des Trainings eines neuronalen Netzes ist es, diese Parameter so zu wählen, dass das Netz die gesuchte Funktion (hier \( u \)) möglichst gut approximiert.


\subsection{Formulierung als Optimierungsproblem}\label{neuronal:subsection:optimierungsproblem}
Durch Subtrahieren der rechten Seite von der Wellengleichung \eqref{neuronal:wellengleichung} erhält man
\begin{equation}
    \frac{\partial^2 u}{\partial t^2} - c^2 \left( \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} \right) = 0.
\end{equation}
Substituiert man nun das neuronale Netz \eqref{neuronal:nn} für \( u \) und quadriert anschliessend, lässt sich das Training des Netzes als Optimierungsproblem formulieren:\newline

Wähle \( \vartheta \) so, dass
\begin{equation}
    \left(\frac{\partial^2 \hat{u}}{\partial t^2} - c^2 \left( \frac{\partial^2 \hat{u}}{\partial x^2} + \frac{\partial^2 \hat{u}}{\partial y^2} \right)\right)^2
    \label{neuronal:optimierung}
\end{equation}
minimal wird.\newline

Durch das Quadrieren wird erreicht, dass der Term für alle \( x, y, t \) immer positiv ist.
Somit sind Minima des Terms tiefstens 0 und nicht beliebig klein.

Sobald eine Lösung \( \vartheta \) zu diesem Problem gefunden wurde, ist
\begin{equation}
    \left(\frac{\partial^2 \hat{u}}{\partial t^2} - c^2 \left( \frac{\partial^2 \hat{u}}{\partial x^2} + \frac{\partial^2 \hat{u}}{\partial y^2} \right)\right)^2 \approx 0
    \iff
    \frac{\partial^2 \hat{u}}{\partial t^2} - c^2 \left( \frac{\partial^2 \hat{u}}{\partial x^2} + \frac{\partial^2 \hat{u}}{\partial y^2} \right) \approx 0
    \iff
    \frac{\partial^2 \hat{u}}{\partial t^2} \approx c^2 \left( \frac{\partial^2 \hat{u}}{\partial x^2} + \frac{\partial^2 \hat{u}}{\partial y^2} \right)
\end{equation}
und die Wellengleichung wird somit approximativ durch das neuronale Netz gelöst.


\subsection{Struktur des neuronalen Netzes}\label{neuronal:subsection:struktur_nn}

Im vorangegangenen Kapitel wurde nicht weiter auf die Struktur des neuronalen Netzes \eqref{neuronal:nn} eingegangen.
Dies wird innerhalb dieses Kapitels gemacht.

Generell können neuronale Netze als Kompositionen von mehreren Teilfunktionen betrachtet werden.
Dabei besteht jede Teilfunktion \( f_i \) aus einer Affintransformation, gefolgt von einer nicht-linearen Aktivierungsfunktion \( g_i \)

\begin{align*}
    f_i\colon \mathbb{R}^n & \longrightarrow\mathbb{R}^m \\[-1ex]
    v & \longmapsto g_i(A_iv + b_i)
\end{align*}

mit \( v \in \mathbb{R}^n, A_i \in \mathbb{R}^{m \times n}, b_i \in \mathbb{R}^m \). 
Die Elemente aller \( A_i \) und \( b_i \) bilden den Vektor \( \vartheta \) der \emph{trainierbaren Parameter}.

Da das Resultat der Affintransformation auch ein Vektor ist, wird die Aktivierungsfunktion \( g_i \) einzeln auf jeden Komponenten angewendet.
Für \( g_i \) gibt es viele verschiedene Möglichkeiten, eine häufig verwendete Aktivierungsfunktion ist die hyperbolische Tangens \( \tanh() \).

Das gesamte neuronale Netzwerk ist somit gegeben als
\begin{equation}
    \hat{u}(x, y, t; \vartheta) = f_k(\ldots(f_i(\ldots(f_1(x, y, t))))) = f_k \circ \ldots \circ f_i \ldots \circ f_1(x, y, t).
    \label{neuronal:nn_ausformuliert}
\end{equation}

Die Domänen und Bereiche der Teilfunktionen \( f_i \) sind mehr oder weniger frei wählbar.
Damit es mit den Dimensionen aufgeht, muss jeweils der Bereich von \( f_i \) gleich der Domäne von \( f_{i+1} \) sein.
Ebenfalls ist man relativ frei in der Wahl der Anzahl Teilfunktion.

Die einzigen strikten Vorgaben sind die Domäne der ersten Teilfunktion \( f_1 \) und der Bereich der letzten Teilfunktion \( f_k \).
Da das Netzwerk die Wellenfunktion
\begin{align*}
    u\colon \mathbb{R}^3 & \longrightarrow\mathbb{R}
\end{align*}

in zwei räumlichen und der zeitlichen Dimension approximieren soll, muss die Domäne von \( f_1, \mathbb{R}^3\) sein und der Bereich von \( f_k, \mathbb{R}\).
