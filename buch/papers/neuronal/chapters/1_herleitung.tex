%
% 1_herleitung.tex -- Herleitung der Methode
%
% (c) 2025 Roman Cvijanovic & Nicola Dall'Acqua, Hochschule Rapperswil
%
% !TEX root = ../../buch.tex
% !TEX encoding = UTF-8
%

\section{Herleitung der Methode\label{neuronal:section:herleitung}}
\kopfrechts{Herleitung der Methode}

Im Folgenden wird die Methode zum Lösen von Feldgleichungen mittels eines neuronalen Netzes theoretisch hergeleitet.
Dies wird anhand des Beispiels der Wellengleichung in zwei räumlichen Dimensionen gemacht
\begin{equation}
    \frac{\partial^2 u}{\partial t^2} = c^2 \left( \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} \right).
    \label{neuronal:wellengleichung}
\end{equation}

Wobei \( u(x, y, t) \) die z-Koordinate am Punkt \( (x, y) \) zum Zeitpunkt \( t \) darstellt.
Anders ausgedrückt ist \( u \) eine Oberfläche, welche sich im Laufe der Zeit ändert und somit eine Welle modelliert. 
Zudem ist \( c \in \mathbb{R} \) eine Konstante und stellt die Verbreitungsgeschwindigkeit der Welle dar.

Weiter sei das neuronale Netzwerk gegeben als
\begin{equation}
    \hat{u}(x, y, t; \vartheta).
    \label{neuronal:nn}
\end{equation}
Das Netz hängt von den gleichen Variablen ab wie \( u \).
Zusätzlich besitzt es einen Vektor \( \vartheta \in \mathbb{R}^n \) der n \emph{trainierbaren Parameter}.
Das Ziel des Trainings eines neuronalen Netzes ist es, diese Parameter so zu wählen, dass das Netz die gesuchte Funktion (hier \( u \)) möglichst gut approximiert.


\subsection{Formulierung als Optimierungsproblem}\label{neuronal:subsection:optimierungsproblem}
Durch Subtrahieren der rechten Seite von der Wellengleichung \eqref{neuronal:wellengleichung} erhält man
\begin{equation}
    \frac{\partial^2 u}{\partial t^2} - c^2 \left( \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} \right) = 0.
\end{equation}
Substituiert man nun das neuronale Netz \eqref{neuronal:nn} für \( u \) und quadriert anschliessend, lässt sich das Training des Netzes als Optimierungsproblem formulieren:\newline

Wähle \( \vartheta \) so, dass
\begin{equation}
    \left(\frac{\partial^2 \hat{u}}{\partial t^2} - c^2 \left( \frac{\partial^2 \hat{u}}{\partial x^2} + \frac{\partial^2 \hat{u}}{\partial y^2} \right)\right)^2
    \label{neuronal:optimierung}
\end{equation}
minimal wird.\newline

Durch das Quadrieren wird erreicht, dass der Term für alle \( x, y, t \) immer positiv ist.
Somit sind Minima des Terms tiefstens 0 und nicht beliebig klein.

Sobald eine Lösung \( \vartheta \) zu diesem Problem gefunden wurde, ist
\begin{equation}
    \left(\frac{\partial^2 \hat{u}}{\partial t^2} - c^2 \left( \frac{\partial^2 \hat{u}}{\partial x^2} + \frac{\partial^2 \hat{u}}{\partial y^2} \right)\right)^2 \approx 0
    \iff
    \frac{\partial^2 \hat{u}}{\partial t^2} - c^2 \left( \frac{\partial^2 \hat{u}}{\partial x^2} + \frac{\partial^2 \hat{u}}{\partial y^2} \right) \approx 0
    \iff
    \frac{\partial^2 \hat{u}}{\partial t^2} \approx c^2 \left( \frac{\partial^2 \hat{u}}{\partial x^2} + \frac{\partial^2 \hat{u}}{\partial y^2} \right)
\end{equation}
und die Wellengleichung wird somit approximativ durch das neuronale Netz gelöst.


\subsection{Struktur des neuronalen Netzes}\label{neuronal:subsection:struktur_nn}

Im vorangegangenen Kapitel wurde nicht weiter auf die Struktur des neuronalen Netzes \eqref{neuronal:nn} eingegangen.
Diese wird innerhalb dieses Kapitels entwickelt.

Generell können neuronale Netze als Kompositionen von mehreren Teilfunktionen betrachtet werden.
Dabei besteht jede Teilfunktion \( f_i \) aus einer Affintransformation, gefolgt von einer nicht-linearen Aktivierungsfunktion \( g_i \)

\begin{align*}
    f_i\colon \mathbb{R}^q & \longrightarrow\mathbb{R}^p \\[-1ex]
    v & \longmapsto g_i(A_iv + b_i)
\end{align*}

mit \( v \in \mathbb{R}^q, A_i \in \mathbb{R}^{p \times q}, b_i \in \mathbb{R}^p \). 
Die Elemente aller \( A_i \) und \( b_i \) bilden den Vektor \( \vartheta \) der \emph{trainierbaren Parameter}.

Weiter ist die Aktivierungsfunktion \( g_i\colon \mathbb{R} \longrightarrow\mathbb{R} \), da das Resultat der Affintransformation ein Vektor ist, wird \( g_i \) einzeln auf jeden Vektor-Komponenten angewendet.
Für \( g_i \) gibt es viele verschiedene Möglichkeiten, eine häufig verwendete Aktivierungsfunktion ist die hyperbolische Tangens \( \tanh() \).

Das gesamte neuronale Netzwerk ist somit gegeben als
\begin{equation}
    \hat{u}(x, y, t; \vartheta) = f_k(\ldots(f_i(\ldots(f_1(x, y, t))))) = f_k \circ \ldots \circ f_i \ldots \circ f_1(x, y, t).
    \label{neuronal:nn_ausformuliert}
\end{equation}

Die Domänen und Bereiche der Teilfunktionen \( f_i \) sind mehr oder weniger frei wählbar.
Damit es mit den Dimensionen aufgeht, muss jeweils der Bereich von \( f_i \) gleich der Domäne von \( f_{i+1} \) sein.
Ebenfalls ist man relativ frei in der Wahl der Anzahl Teilfunktion.

Die einzigen strikten Vorgaben sind die Domäne der ersten Teilfunktion \( f_1 \) und der Bereich der letzten Teilfunktion \( f_k \).
Da das Netzwerk die Wellenfunktion
\begin{align*}
    u\colon \mathbb{R}^3 & \longrightarrow\mathbb{R}
\end{align*}

in zwei räumlichen und der zeitlichen Dimension approximieren soll, muss die Domäne von \( f_1, \mathbb{R}^3\) sein und der Bereich von \( f_k, \mathbb{R}\).

Je mehr Teilfunktionen und Anzahl Dimensionen in den Teilfunktionen, desto mehr trainierbare Parameter besitzt das Modell.
Auf die Frage nach der Anzahl Parameter gibt es keine ``richtige'' Antwort.
Als Faustregel gilt: Je komplizierter die Funktion die approximiert wird, desto mehr Parameter sollten verwendet werden.

% TODO: Auf Struktur/Anzahl Parameter des verwendeten Netzes für die Wellengleichung eingehen (sobald dieses fertig ist)


\subsection{Training des neuronalen Netzes}\label{neuronal:subsection:training_nn}

Nachdem nun das neuronale Netzwerk als Funktion definiert ist, ist der nächste Schritt die Wahl von \( \vartheta \) bzw. die Wahl der trainierbaren Parameter.
Dies wird als Training des Netzes bezeichnet.

Wie im Kapitel \ref{neuronal:subsection:optimierungsproblem} beschrieben, soll das Netz den Term \eqref{neuronal:optimierung} minimieren.
Hierzu werden drei Dinge benötigt:
\begin{itemize}
    \item Ein Trainings-Datensatz an x-, y- und t-Werten
    \item Eine Loss-Funktion
    \item Ein Optimierungsalgorithmus\\
\end{itemize}

\textbf{Trainings-Datensatz}\\
Der Trainings-Datensatz muss aus x-, y- und t-Werten bestehen, für die die Wellengleichung erfüllt ist.
Da die Wellengleichung für alle x, y und t gilt, können zufällig generierte, reelle Zahlen verwendet werden.
Grundsätzlich gilt, je mehr Parameter das neuronale Netz hat, desto mehr Werte benötigt man.
Sei die Anzahl Werte \( k \), so ist der Datensatz:
\begin{center}
    \( x_1, x_2, \ldots, x_k \)\\
    \( y_1, y_2, \ldots, y_k \)\\
    \( t_1, t_2, \ldots, t_k \)\\
\end{center}


% TODO: In welchem Bereich werden die Zahlenwerte generiert und wieviele genau?

\textbf{Loss-Funktion}\\
Die Loss-Funktion ist eine Funktion von \( \vartheta \) und liefert eine reelle Zahl als Resultat.
Diese reelle Zahl ist ein Mass dafür, wie gut das neuronale Netzwerk, mit den verwendeten Parametern, die Wellengleichung an den x-, y- und t-Werten aus dem Trainings-Datensatzes approximiert.
Die Loss-Funktion ist somit die Summe von \eqref{neuronal:optimierung} über alle Datensatz-Werte.

Sei die Loss-Funktion
\begin{equation}
    J(\vartheta) = \sum_{i=1}^{k} \left(\frac{\partial^2 \hat{u}}{\partial t_i^2} - c^2 \left( \frac{\partial^2 \hat{u}}{\partial x_i^2} + \frac{\partial^2 \hat{u}}{\partial y_i^2} \right)\right)^2.
    \label{neuronal:loss}
\end{equation}

Da die Funktion \( J \) eine Summe von Parabeln mit positiven Vorzeichen ist, kann sie minimal 0 werden.

\textbf{Optimierungsalgorithmus}\\
Der Optimierungsalgorithmus hat zum Ziel, die Loss-Funktion \( J \) zu minimieren.
Da \( J \) ein hochdimensionales Hyper-Surface ist, ist es sehr unwahrscheinlich dass man analytisch ein Minimum findet.
Das bedeutet, dass stattdessen ein numerischer Algorithmus verwendet werden muss, welcher Minima approximativ findet.

Die Schritte die der Algorithmus durchläuft sind:
\begin{enumerate}
    \item Werte \( J \) an den aktuellen Parameterwerten in \( \vartheta \) aus. Dies ergibt eine Zahl \( r_1 \)
    \item Passe die Parameterwerte leicht an, so dass \( J \) an den neuen Parameterwerten eine Zahl \( r_2 \) mit \( r_2 \leq r_1 \) ergibt
    \item Wiederhole dies solange, bis J genügend nahe bei 0 ist. (J = 0 würde bedeuten dass das Netzwerk die Wellengleichung an den Punkten des Trainings-Datensatzes perfekt löst.)
\end{enumerate}

Nach dem letzten Schritt dieses Algorithmus ist das neuronale Netzwerk fertig trainiert.
%% TODO: Wie gut ist die Approximierung und von was hängt die Qualität ab?
